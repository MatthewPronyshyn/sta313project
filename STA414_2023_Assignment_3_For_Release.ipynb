{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-bf1iLXTkXd5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewPronyshyn/sta313project/blob/main/STA414_2023_Assignment_3_For_Release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changelog:  (Last Updated 2023-03-24)\n",
        "\n",
        "* Added re-initializing parameters in 1.e"
      ],
      "metadata": {
        "id": "gMAXaY8USfWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic ML: Assignment 3\n",
        "- **Deadline**: 2023-03-27 (March 27th 2023)\n",
        "- **Submission**: You need to submit your solutions through Crowdmark, including all your derivations, plots, and your code. You can produce the files however you like (e.g. $\\LaTeX$, Microsoft Word, etc), as long as it is readable. Points will be deducted if we have a hard time reading your solutions or understanding the structure of your code.\n",
        "- **Collaboration policy**: After attempting the problems on an individual basis, you may discuss and work together on the assignment with up to two classmates. However, **you must write your own code and write up your own solutions individually and explicitly name any collaborators** at the top of the homework."
      ],
      "metadata": {
        "id": "248_qtulv8jx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1RKToQQcX2Q"
      },
      "source": [
        "# 1. [50pts] Stochastic Variational Inference in the TrueSkill Model\n",
        "\n",
        "## Background\n",
        "\n",
        "We'll continue working with [TrueSkill](http://papers.nips.cc/paper/3079-trueskilltm-a-bayesian-skill-rating-system.pdf) model, a player ranking system for competitive games originally developed for Halo 2. Recall the model:\n",
        "\n",
        "\n",
        "## Model definition\n",
        "\n",
        "We assume that each player has a true, but unknown skill $z_i \\in \\mathbb{R}$.\n",
        "We use $N$ to denote the number of players.\n",
        "\n",
        "### The prior:\n",
        "The prior over each player's skill is a standard normal distribution, and all player's skills are *a priori* independent.\n",
        "\n",
        "### The likelihood:\n",
        "For each observed game, the probability that player $i$ beats player $j$, given the player's skills $z_A$ and $z_B$, is:\n",
        "$$p(A \\,\\, \\text{beat} \\,\\, B | z_A, z_B) = \\sigma(z_i - z_j)$$\n",
        "where\n",
        "$$\\sigma(y) = \\frac{1}{1 + \\exp(-y)}$$\n",
        "We chose this function simply because it's close to zero or one when the player's skills are very different, and equals one-half when the player skills are the same.  This likelihood function is the only thing that gives meaning to the latent skill variables $z_1 \\dots z_N$.\n",
        "\n",
        "There can be more than one game played between a pair of players. The outcome of each game is independent given the players' skills.\n",
        "We use $M$ to denote the number of games.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6euHm_h4Voi"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import wget\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import scipy.io\n",
        "import scipy.stats\n",
        "import torch \n",
        "import random\n",
        "from torch import nn\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import trange, tqdm_notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper function\n",
        "def diag_gaussian_log_density(x, mu, std):\n",
        "    # axis=-1 means sum over the last dimension.\n",
        "    m = Normal(mu, std)\n",
        "    return torch.sum(m.log_prob(x), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bf1iLXTkXd5"
      },
      "source": [
        "## Implementing the TrueSkill Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A62Y2IWg39ix"
      },
      "source": [
        "This part was mostly done in Assignment 2. We will recall some useful functions.\n",
        "\n",
        "**a)** The function $\\texttt{log_joint_prior}$ computes the log of the prior, jointly evaluated over all player's skills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDmHDiac4Rpr"
      },
      "outputs": [],
      "source": [
        "def log_joint_prior(zs_array):\n",
        "    return diag_gaussian_log_density(zs_array, torch.tensor([0.0]), torch.tensor([1.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAbAgxMl4X2q"
      },
      "source": [
        "**b)** The function `logp_a_beats_b` that, given a pair of skills $z_a$ and $z_b$, evaluates the log-likelihood that player with skill $z_a$ beat player with skill $z_b$ under the model detailed above.\n",
        "\n",
        "To ensure numerical stability, we use the function `np.log1p` that computes $\\log(1 + x)$ in a numerically stable way.  Or even better, use `np.logaddexp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7nUXkYl_f4T"
      },
      "outputs": [],
      "source": [
        "def logp_a_beats_b(z_a, z_b):\n",
        "    return -torch.logaddexp(torch.tensor([0.0]), z_b - z_a)\n",
        "\n",
        "def log_prior_over_2_players(z1, z2):\n",
        "    m = Normal(torch.tensor([0.0]), torch.tensor([[1.0]]))\n",
        "    return m.log_prob(z1) + m.log_prob(z2)\n",
        "\n",
        "def prior_over_2_players(z1, z2):\n",
        "    return torch.exp(log_prior_over_2_players(z1, z2))\n",
        "\n",
        "def log_posterior_A_beat_B(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) + logp_a_beats_b(z1, z2)\n",
        "\n",
        "def posterior_A_beat_B(z1, z2):\n",
        "    return torch.exp(log_posterior_A_beat_B(z1, z2))\n",
        "\n",
        "def log_posterior_A_beat_B_10_times(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) + 10.0 * logp_a_beats_b(z1, z2)\n",
        "\n",
        "def posterior_A_beat_B_10_times(z1, z2):\n",
        "    return torch.exp(log_posterior_A_beat_B_10_times(z1, z2))\n",
        "\n",
        "def log_posterior_beat_each_other_10_times(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) \\\n",
        "        + 10.* logp_a_beats_b(z1, z2) \\\n",
        "        + 10.* logp_a_beats_b(z2, z1)\n",
        "\n",
        "def posterior_beat_each_other_10_times(z1, z2):\n",
        "    return torch.exp(log_posterior_beat_each_other_10_times(z1, z2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions will be used for plotting.  Note that `plot_2d_fun` can now take an optional second function, so you can compare two functions."
      ],
      "metadata": {
        "id": "hA77ZVR5SgGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at7JEyWs_QRK"
      },
      "outputs": [],
      "source": [
        "# Plotting helper functions for free\n",
        "def plot_isocontours(ax, func, xlimits=[-4, 4], ylimits=[-4, 4], steps=101, cmap=\"summer\"):\n",
        "    x = torch.linspace(*xlimits, steps=steps)\n",
        "    y = torch.linspace(*ylimits, steps=steps)\n",
        "    X, Y = torch.meshgrid(x, y)\n",
        "    Z = func(X, Y)\n",
        "    plt.contour(X, Y, Z, cmap=cmap)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "\n",
        "def plot_2d_fun(f, x_axis_label=\"\", y_axis_label=\"\", f2=None, scatter_pts=None):\n",
        "    # This is the function your code should call.\n",
        "    # f() should take two arguments.\n",
        "    fig = plt.figure(figsize=(8,8), facecolor='white')\n",
        "    ax = fig.add_subplot(111, frameon=False)\n",
        "    ax.set_xlabel(x_axis_label)\n",
        "    ax.set_ylabel(y_axis_label)\n",
        "    plot_isocontours(ax, f)\n",
        "    if f2 is not None:\n",
        "      plot_isocontours(ax, f2, cmap='winter')\n",
        "    \n",
        "    if scatter_pts is not None:\n",
        "      plt.scatter(scatter_pts[:,0], scatter_pts[:, 1])\n",
        "    plt.plot([4, -4], [4, -4], 'b--')   # Line of equal skill\n",
        "    plt.show(block=True)\n",
        "    plt.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm-SM6Fc4yz8"
      },
      "source": [
        "## **1.1 [20pts]** Stochastic Variational Inference on Two Players and Toy Data\n",
        "\n",
        "One nice thing about a Bayesian approach is that it separates the model specification from the approximate inference strategy.\n",
        "The original Trueskill paper from 2007 used message passing.\n",
        "\n",
        "In this question we will  approximate posterior distributions with gradient-based stochastic variational inference.\n",
        "\n",
        "The parameters are $\\phi = (\\mu,\\log(\\sigma))$. Notice that instead of $\\sigma$ (which is constrained to be positive), we work with $\\log(\\sigma)$, removing the constraint. This way, we can do unconstrained gradient-based optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ri8HRQ461m"
      },
      "source": [
        "**a) [9pts]** Implement the missing lines in the below code, to complete the evidence lower bound function and the reparameterized sampler for the approximate posterior.\n",
        "\n",
        "Hint 1: You must use the reparametrization trick in your sampler if you want your gradients to be unbiased.\n",
        "\n",
        "Hint 2: If you're worried you got these wrong, you can check that the sampler matches the log pdf by plotting a histogram of samples against a plot of the pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZlCOViq5Ahf"
      },
      "outputs": [],
      "source": [
        "def diag_gaussian_samples(mean, log_std, num_samples):\n",
        "    # mean and log_std are (D) dimensional vectors\n",
        "    # Return a (num_samples, D) matrix, where each sample is\n",
        "    # from a diagonal multivariate Gaussian.\n",
        "\n",
        "    # TODO.  You might want to use torch.randn(). Remember\n",
        "    # you must use the reparameterization trick.  Also remember that\n",
        "    # we are parameterizing the _log_ of the standard deviation.\n",
        "\n",
        "    return \n",
        "\n",
        "\n",
        "def diag_gaussian_logpdf(x, mean, log_std):\n",
        "    # Evaluate the density of a batch of points on a \n",
        "    # diagonal multivariate Gaussian. x is a (num_samples, D) matrix.\n",
        "    # Return a tensor of shape (num_samples)\n",
        "    \n",
        "    return\n",
        "\n",
        "def batch_elbo(logprob, mean, log_std, num_samples):\n",
        "    # TODO: Use simple Monte Carlo to estimate ELBO\n",
        "    # on a batch of size num_samples\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfAD3ffb5CD9"
      },
      "source": [
        "\n",
        "**b) [4pts]** Write a loss function called $\\texttt{objective}$  that takes variational distribution parameters, and returns an unbiased estimate of the negative elbo using $\\texttt{num_samples_per_iter}$ samples, to approximate the joint posterior over skills conditioned on observing player A winning 10 games.\n",
        "\n",
        "Note: We want a _negative_ ELBO estimate, because the convention in optimization is to minimize functions, and we want to maximize the ELBO."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_players = 2\n",
        "n_iters = 800\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 50\n",
        "\n",
        "def log_posterior_A_beat_B_10_times_1_arg(z1z2):\n",
        "  return log_posterior_A_beat_B_10_times(z1z2[:,0], z1z2[:,1]).flatten()\n",
        "\n",
        "def objective(params):  # The loss function to be minimized.\n",
        "  # TODO.  Hint:  This can be done in one line.\n",
        "  return "
      ],
      "metadata": {
        "id": "j-Qt2vqvKgAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) [1pts]** Initialize a set of variational parameters and optimize them to approximate the joint where we observe player A winning 10 games. Report the final loss. Also plot the optimized variational approximation contours and the target distribution on the same axes.\n",
        "\n",
        "Hint:  Almost initialization should be fine.  How many variational parameters do you need?"
      ],
      "metadata": {
        "id": "ixGDUDjwKnWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biW5JiZq5EXc"
      },
      "outputs": [],
      "source": [
        "def callback(params, t):\n",
        "  if t % 25 == 0:\n",
        "    print(\"Iteration {} lower bound {}\".format(t, objective(params)))\n",
        "\n",
        "# Set up optimizer.\n",
        "D = 2\n",
        "# init_log_std  = # TODO.\n",
        "# init_mean = # TODO\n",
        "\n",
        "params = (init_mean, init_log_std)\n",
        "optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n",
        "\n",
        "def update():\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective(params)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Main loop.\n",
        "print(\"Optimizing variational parameters...\")\n",
        "for t in trange(0, n_iters):\n",
        "    update()\n",
        "    callback(params, t)\n",
        "\n",
        "\n",
        "def approx_posterior_2d(z1, z2):\n",
        "    # The approximate posterior\n",
        "    mean, logstd = params[0].detach(), params[1].detach()\n",
        "    return torch.exp(diag_gaussian_logpdf(torch.stack([z1, z2], dim=2), mean, logstd))\n",
        "\n",
        "plot_2d_fun(posterior_A_beat_B_10_times, \"Player A Skill\", \"Player B Skill\",\n",
        "            f2=approx_posterior_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCmqGbeK5iAf"
      },
      "source": [
        "**d) [3pts]** Write a loss function called $\\texttt{objective}$  that takes variational distribution parameters , and returns a negative elbo estimate using simple Monte carlo with $\\texttt{num_samples_per_iter}$ samples, to approximate the joint where we observe player A winning 10 games and player B winning 10 games.\n",
        "\n",
        "Hint:  You can find analogous functions in the code above.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "n_iters = 100\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 50\n",
        "\n",
        "def log_posterior_beat_each_other_10_times_1_arg(z1z2):\n",
        "    # z1z2 is a tensor with shape (num_samples x 2)\n",
        "    # Return a tensor with shape (num_samples)\n",
        "\n",
        "    return\n",
        "\n",
        "def objective(params):\n",
        "    return"
      ],
      "metadata": {
        "id": "5Zz6DkOacXDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e) [3pts]** Run the code below to optimize, and report the final loss. Also plot the optimized variational approximation contours and the target distribution on the same axes.\n",
        "\n",
        "---\n",
        "\n",
        "Write one or two sentences describing the joint settings of skills that are plausible under the true posterior, but which are not plausible under the approximate posterior.\n",
        "\n",
        "--- \n",
        "\n",
        "Finally, answer with one or two sentences:  Would changing the variational approximate posterior from a fully-factorized (diagonal covariance) Gaussian to a non-factorized (fully parameterized covariance) Gaussian make a better approximation in this instance?"
      ],
      "metadata": {
        "id": "_V42MMIscvMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksjggALU5gWx"
      },
      "outputs": [],
      "source": [
        "# Main loop. \n",
        "# Reinitialize with the same initialization as you did in 1c) . \n",
        "init_mean = # TODO\n",
        "init_log_std = #TODO \n",
        "params = (init_mean, init_log_std)\n",
        "optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n",
        "\n",
        "print(\"Optimizing variational parameters...\")\n",
        "for t in trange(0, n_iters):\n",
        "    update()\n",
        "    callback(params, t)\n",
        "\n",
        "plot_2d_fun(posterior_beat_each_other_10_times, \"Player A Skill\", \"Player B Skill\",\n",
        "            f2=approx_posterior_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrCmtd_b51Jf"
      },
      "source": [
        "## 1.2 [30 pts] Approximate inference conditioned on real data \n",
        "\n",
        "The dataset contains data on 2546 chess games amongst 1434 players:\n",
        " - names is a 1434 by 1 matrix, whose $i$’th entry is the name of player $i$.\n",
        " - games is a 2546 by 2 matrix of game outcomes (actually chess matches), one row per game.\n",
        "\n",
        "The first column contains the indices of the players who won.\n",
        "The second column contains the indices of the player who lost.\n",
        "\n",
        "It is based on the kaggle chess dataset: https://www.kaggle.com/datasets/datasnaek/chess\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dbf_GOZ7ejb"
      },
      "outputs": [],
      "source": [
        "wget.download(\"https://michalmalyska.github.io/csc412/chess_games.csv\")\n",
        "games = pd.read_csv(\"chess_games.csv\")[[\"winner_index\", \"loser_index\"]].to_numpy()\n",
        "wget.download(\"https://michalmalyska.github.io/csc412/chess_players.csv\")\n",
        "names = pd.read_csv(\"chess_players.csv\")[[\"index\", \"player_name\"]].to_numpy()\n",
        "\n",
        "games = torch.LongTensor(games)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkYB8hS07fOW"
      },
      "source": [
        "\n",
        "\n",
        "**a) [0pt]** Assuming all game outcomes are i.i.d. conditioned on all players' skills, the function $\\texttt{log_games_likelihood}$ takes a batch of player skills $\\texttt{zs}$ and a collection of observed games $\\texttt{games}$ and gives the total log-likelihood for all those observations given all the skills.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkW93RBf7pru"
      },
      "outputs": [],
      "source": [
        "def log_games_likelihood(zs, games):\n",
        "    winning_player_ixs = games[:,0]\n",
        "    losing_player_ixs = games[:,1]\n",
        "\n",
        "    winning_player_skills = zs[:, winning_player_ixs] \n",
        "    losing_player_skills = zs[:, losing_player_ixs]\n",
        "\n",
        "    log_likelihoods = logp_a_beats_b(winning_player_skills, losing_player_skills)\n",
        "    return torch.sum(log_likelihoods, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaPEulQoAndL"
      },
      "outputs": [],
      "source": [
        "def log_joint_probability(zs):\n",
        "    return log_joint_prior(zs) + log_games_likelihood(zs, games)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUWILTA18BRn"
      },
      "source": [
        "**b) [4pt]** Write a new objective function like the one from the previous question. \n",
        "\n",
        "Below, we initialize a variational distribution and fit it to the joint distribution with all the observed tennis games from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_players = 1434\n",
        "n_iters = 500\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 150\n",
        "\n",
        "def objective(params):\n",
        "    return"
      ],
      "metadata": {
        "id": "qsCQUJCFd2Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) [3pts]** Optimize, and report the final loss. "
      ],
      "metadata": {
        "id": "O3ep43C5d_CT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ock35XuW8EK8"
      },
      "outputs": [],
      "source": [
        "# Set up optimizer.\n",
        "init_mean = torch.zeros(num_players, requires_grad=True)\n",
        "init_log_std  = torch.zeros(num_players, requires_grad=True)\n",
        "params = (init_mean, init_log_std)\n",
        "optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n",
        "\n",
        "def update():\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective(params)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Optimize and print loss in a loop\n",
        "# HINT: you can use the callback() function to report loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6YluZ2B8Qmr"
      },
      "source": [
        "**d) [1pt]** Plot the approximate mean and variance of all players, sorted by skill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEZtYBeH8TOO"
      },
      "outputs": [],
      "source": [
        "# mean_skills, logstd_skills = # TODO.  Hint: You don't need to do simple Monte Carlo here.\n",
        "# Hint: You should use .detach() before you do anything with the params tensors\n",
        "mean_skills, logstd_skills = \n",
        "order = torch.argsort(mean_skills)\n",
        "\n",
        "plt.xlabel(\"Player Rank\")\n",
        "plt.ylabel(\"Player Skill\")\n",
        "plt.errorbar(range(num_players), )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjWom7_Z8VCY"
      },
      "source": [
        "**e) [1pts]** List the names of the 10 players with the highest mean skill under the variational model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LCPM5Bp8YiK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEj6LkYA8ncH"
      },
      "source": [
        "**f) [3pt]** Plot samples from the joint posterior over the skills of lelik3310 and thebestofthebad. Based on your samples, describe in a sentence the relationship between the skills of the players. (Is one better than the other? Are they approximately even?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6zI2sCB8qFO"
      },
      "outputs": [],
      "source": [
        "lelik3310_ix = 496\n",
        "thebestofthebad_ix = 512\n",
        "print(names[lelik3310_ix])\n",
        "print(names[thebestofthebad_ix])\n",
        "\n",
        "fig = plt.figure(figsize=(8,8), facecolor='white')\n",
        "\n",
        "# Label each with \"<player> Skill\"\n",
        "plt.xlabel(\"lelik3310 Skill\") \n",
        "plt.ylabel(\"thebestofthebad Skill\") \n",
        "\n",
        "plt.plot([3, -3], [3, -3], 'b--') # Line of equal skill\n",
        "\n",
        "samples = diag_gaussian_samples(mean_skills, logstd_skills, 100)\n",
        "\n",
        "# TODO:  Hint:  Use plt.scatter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUXcsAYdDnor"
      },
      "source": [
        "**g) [6pts]** Derive the exact probability under a factorized Gaussian over two players’ skills that one has higher skill than the other, as a function of the two means and variances over their skills. Express your answer in terms of the cumulative distribution function of a one-dimensional Gaussian random variable.\n",
        "\n",
        "- Hint 1: Use a linear change of variables $y_A, y_B = z_A − z_B , z_B$. What does the line of equal skill look like after this transformation?\n",
        "- Hint 2: If $X \\sim N (\\mu, \\Sigma)$, then $AX \\sim  N (A\\mu, A\\Sigma A^T)$ where $A$ is a linear transformation.\n",
        "- Hint 3: Marginalization in Gaussians is easy: if $X \\sim N (\\mu, \\Sigma)$, then the $i$th element of $X$ has a\n",
        "marginal distribution $X_i \\sim N (\\mu_i , \\Sigma_{ii})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxazFdqcFhF_"
      },
      "source": [
        "Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO3k7rfI8sWg"
      },
      "source": [
        "**h) [4pts]** Compute the probability under your approximate posterior that lelik3310 has higher skill than thebestofthebad. Compute this quantity exactly using the formula you just derived above, and also estimate it using simple Monte Carlo with 10000 examples.\n",
        "\n",
        "Hint:  You might want to use `Normal(0,1).cdf()` for the exact formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqXgDi-T-W7o"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def prob_A_superior_B(N, A_ix, B_ix):\n",
        "    \n",
        "\n",
        "    return formula_est, mc_est\n",
        "\n",
        "formula_est, mc_est = prob_A_superior_B(10000, lelik3310_ix, thebestofthebad_ix)\n",
        "print(f\"Exact CDF Estimate: {formula_est}\")\n",
        "print(f\"Simple MC Estimate: {mc_est}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olpPTnm3-YdN"
      },
      "source": [
        "**i) [2pts]** Compute the probability that lelik3310 is better than the player with the 5th lowest mean skill. Compute this quantity exactly, and then estimate it using simple Monte Carlo with 10000 examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "fifth_worst_ix = \n"
      ],
      "metadata": {
        "id": "PNLfQCDdbZ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**j) [3 pts]** Imagine that we knew ahead of time that we were examining the skills of top chess players, and so changed our prior on all players to Normal(10, 1) and re-ran our approximate inference from scratch. Would that change the answer of either of the previous 2 questions, in expectation?"
      ],
      "metadata": {
        "id": "kjslZo7WUH2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here."
      ],
      "metadata": {
        "id": "W37pO-PDNjKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k) [3 pts]** Based on all the plots and results in this assignment and HW2, which approximate inference method do you suspect is producing a better overall approximation to the true posterior over all skills conditioned on all games?  Give a short explanation."
      ],
      "metadata": {
        "id": "cNJ5WuyQKPPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here."
      ],
      "metadata": {
        "id": "mtmXbA3OKSoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YIWDBx63PYmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. [50pts] Expectation-Maximization (EM) algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "Ru-EwGWu5T4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import scipy\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ttc_nnUN5Wnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 [7pts] Generating the Data\n",
        "\n",
        "**a) [5pts]** First, we will generate some data for this problem. Set the number of points $N=400$, their dimension $D=2$, and the number of clusters $K=2$, and generate data from the distribution $p(x|z=k) = N(\\mu_k, \\Sigma_k)$.\n",
        "  Sample $200$ data points for $k=1$ and 200 for $k=2$, with\n",
        "\n",
        "  $$\n",
        "    \\mu_1=\n",
        "    \\begin{bmatrix}\n",
        "      0.1 \\\\\n",
        "      0.1\n",
        "    \\end{bmatrix}\n",
        "    \\ \\text{,}\\\n",
        "    \\mu_2=\n",
        "    \\begin{bmatrix}\n",
        "      6.0 \\\\\n",
        "      0.1\n",
        "    \\end{bmatrix}\n",
        "    \\ \\text{ and }\\\n",
        "    \\Sigma_1=\\Sigma_2=\n",
        "    \\begin{bmatrix}\n",
        "      10       & 7 \\\\\n",
        "      7 & 10\n",
        "    \\end{bmatrix}\n",
        "  $$\n",
        "  Here, $N=400$. If you generate the data, you already know which sample comes from which class. Complete the following statements and run to generate.\n",
        "\n",
        "  Hint: you can use `np.random.multivariate_normal`."
      ],
      "metadata": {
        "id": "g-aZi8IN5Via"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = # TODO\n",
        "cov = # TODO\n",
        "mean_1 = # TODO\n",
        "mean_2 = # TODO\n",
        "\n",
        "x_class1 = # TODO\n",
        "x_class2 = # TODO\n",
        "xy_class1 = # TODO\n",
        "xy_class2 = # TODO\n",
        "\n",
        "data_full = # TODO complete data\n",
        "np.random.shuffle(data_full)\n",
        "data = # TODO incomplete data\n",
        "labels = # TODO"
      ],
      "metadata": {
        "id": "E65KHOjz5axT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) [2pts]** Make a scatter plot of the data points showing the true cluster assignment of each point using different color codes and shape (x for first class and circles for second class):"
      ],
      "metadata": {
        "id": "MH5WNKYh5kwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot( # TODO ) # first class, x shape\n",
        "plt.plot( # TODO ) # second class, circle shape"
      ],
      "metadata": {
        "id": "0PZMA--g5mbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 [14pts] Implement and Run K-Means algorithm\n",
        "\n",
        "Now, we assume that the true class labels are not known. Implement the k-means algorithm for this problem.\n",
        "  Write two functions: `km_assignment_step`, and `km_refitting_step` as given in the lecture (Here, `km_` means k-means).\n",
        "  Identify the correct arguments, and the order to run them. Initialize the algorithm with\n",
        "  $$\n",
        "    \\hat{\\mu}_1=\n",
        "    \\begin{bmatrix}\n",
        "      0.0 \\\\\n",
        "      0.0\n",
        "    \\end{bmatrix}\n",
        "    \\ \\text{,}\\\n",
        "    \\hat{\\mu}_2=\n",
        "    \\begin{bmatrix}\n",
        "      1.0 \\\\\n",
        "      1.0\n",
        "    \\end{bmatrix}\n",
        "  $$\n",
        "  and run it until convergence.\n",
        "  Show the resulting cluster assignments on a scatter plot either using different color codes or shape or both.\n",
        "  Also plot the cost (distortion in lecture slides) vs. the number of iterations. Report your misclassification error. "
      ],
      "metadata": {
        "id": "uk3xc1iS5r9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a) [2pts]** Complete the `cost` function."
      ],
      "metadata": {
        "id": "U2yxjnx36Fdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(data, R, Mu):\n",
        "    N, D = # TODO\n",
        "    K = # TODO\n",
        "    # TODO\n",
        "    \n",
        "    return J"
      ],
      "metadata": {
        "id": "4ir5sGuL5nwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) [4pts]** K-Means assignment step."
      ],
      "metadata": {
        "id": "iHeJWG4W6NbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def km_assignment_step(data, Mu):\n",
        "    \"\"\" Compute K-Means assignment step\n",
        "    \n",
        "    Args:\n",
        "        data: a NxD matrix for the data points\n",
        "        Mu: a DxK matrix for the cluster means locations\n",
        "    \n",
        "    Returns:\n",
        "        R_new: a NxK matrix of responsibilities\n",
        "    \"\"\"\n",
        "    \n",
        "    N, D = # TODO: Number of datapoints and dimension of datapoint\n",
        "    K = # number of clusters\n",
        "    r = ... # TODO\n",
        "    # TODO assign all r[:, k]\n",
        "    \n",
        "    arg_min = ... # TODO: argmax/argmin along dimension 1\n",
        "    R_new = ... # TODO: Set to zeros/ones with shape (N, K)\n",
        "    R_new[..., ...] = 1 # TODO Assign to 1\n",
        "    return R_new"
      ],
      "metadata": {
        "id": "A4MF3nyy6Ms8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) [2pts]** K-Means refitting step."
      ],
      "metadata": {
        "id": "v3DnutNm6n33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: K-means Refitting Step\n",
        "def km_refitting_step(data, R, Mu):\n",
        "    \"\"\" Compute K-Means refitting step.\n",
        "    \n",
        "    Args:\n",
        "        data: a NxD matrix for the data points\n",
        "        R: a NxK matrix of responsibilities\n",
        "        Mu: a DxK matrix for the cluster means locations\n",
        "    \n",
        "    Returns:\n",
        "        Mu_new: a DxK matrix for the new cluster means locations\n",
        "    \"\"\"\n",
        "    N, D = # TODO: number of datapoints and dimension of datapoint\n",
        "    K = # TODO: number of clusters\n",
        "    Mu_new = # TODO\n",
        "    return Mu_new"
      ],
      "metadata": {
        "id": "99c-OaVs6tlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d) [4pts]** Run this cell to call the K-Means algorithm."
      ],
      "metadata": {
        "id": "dXJinSwV6xsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D = # TODO\n",
        "K = # TODO\n",
        "max_iter = 100\n",
        "class_init = # TODO\n",
        "R = # TODO\n",
        "\n",
        "Mu = np.zeros([D, K])\n",
        "Mu[:, 1] = 1.\n",
        "R.T.dot(data), np.sum(R, axis=0)\n",
        "\n",
        "for it in range(max_iter):\n",
        "    R = # TODO\n",
        "    Mu = # TODO\n",
        "    print(it, cost(data, R, Mu))\n",
        "\n",
        "class_1 = # TODO\n",
        "class_2 = # TODO"
      ],
      "metadata": {
        "id": "8z_xw_Gu6xJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e) [2pts]** Make a scatterplot for the data points showing the K-Means cluster assignments of each point.\n"
      ],
      "metadata": {
        "id": "MgB7VKUp_gqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(...) # TODO first class, x shape\n",
        "plt.plot(...) # TODO second class, circle shape"
      ],
      "metadata": {
        "id": "SGyd4sKP_jY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 [21pts] Implement EM algorithm for Gaussian mixtures\n",
        "\n",
        "Next, implement the EM algorithm for Gaussian mixtures. Write three functions: `log_likelihood`, `gm_e_step`, and `gm_m_step` as given in the lecture.\n",
        "  - Identify the correct arguments, and the order to run them.\n",
        "  - Initialize the algorithm with the same initialization as in Q2.1 for the means, and with $\\hat\\Sigma_1=\\hat\\Sigma_2=I$, and $\\hat\\pi_1=\\hat\\pi_2$ for the covariances.\n",
        "    \n",
        "Run the algorithm until convergence and show the resulting cluster assignments on a scatter plot either using different color codes or shape or both. Also plot the log-likelihood vs. the number of iterations. Report your misclassification error."
      ],
      "metadata": {
        "id": "aHONjfYq7t-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_density(x, mu, Sigma):\n",
        "    return np.exp(-.5 * np.dot(x - mu, np.linalg.solve(Sigma, x - mu))) \\\n",
        "        / np.sqrt(np.linalg.det(2 * np.pi * Sigma))"
      ],
      "metadata": {
        "id": "Ciq6jLpQ8BWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a) [5pts]** Log-Likelihood."
      ],
      "metadata": {
        "id": "mcVJXu8-8C8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(data, Mu, Sigma, Pi):\n",
        "    \"\"\" Compute log likelihood on the data given the Gaussian Mixture Parameters.\n",
        "    \n",
        "    Args:\n",
        "        data: a NxD matrix for the data points\n",
        "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
        "        Sigma: a list of size K with each element being DxD covariance matrix\n",
        "        Pi: a vector of size K for the mixing coefficients\n",
        "    \n",
        "    Returns:\n",
        "        L: a scalar denoting the log likelihood of the data given the Gaussian Mixture\n",
        "    \"\"\"\n",
        "    # N, D = ...  # TODO Number of datapoints and dimension of datapoint\n",
        "    # K = ... # TODO number of mixtures\n",
        "    L, T = 0., 0.\n",
        "    # TODO: given n, k, compute the likelihood from the k-th Gaussian weighted by the mixing coefficients\n",
        "    return L"
      ],
      "metadata": {
        "id": "iZGUOMuZ8KAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) [5pts]** Gaussian Mixture Expectation Step."
      ],
      "metadata": {
        "id": "E6Km8Q-P8Jr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gm_e_step(data, Mu, Sigma, Pi):\n",
        "    \"\"\" Gaussian Mixture Expectation Step.\n",
        "\n",
        "    Args:\n",
        "        data: a NxD matrix for the data points\n",
        "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
        "        Sigma: a list of size K with each element being DxD covariance matrix\n",
        "        Pi: a vector of size K for the mixing coefficients\n",
        "    \n",
        "    Returns:\n",
        "        Gamma: a NxK matrix of responsibilities \n",
        "    \"\"\"\n",
        "    N, D = ... # TODO Number of datapoints and dimension of datapoint\n",
        "    K = ... # TODO number of mixtures\n",
        "    Gamma = ... # TODO zeros of shape (N,K), matrix of responsibilities\n",
        "\n",
        "    # TODO: given n, k, normalize by sum across second dimension (mixtures)\n",
        "\n",
        "    return Gamma"
      ],
      "metadata": {
        "id": "EOy1VrrS8bPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) [5pts]** Gaussian Mixture Maximization Step."
      ],
      "metadata": {
        "id": "bHuKzKLD8yjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gm_m_step(data, Gamma):\n",
        "    \"\"\" Gaussian Mixture Maximization Step.\n",
        "\n",
        "    Args:\n",
        "        data: a NxD matrix for the data points\n",
        "        Gamma: a NxK matrix of responsibilities \n",
        "    \n",
        "    Returns:\n",
        "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
        "        Sigma: a list of size K with each element being DxD covariance matrix\n",
        "        Pi: a vector of size K for the mixing coefficients\n",
        "    \"\"\"\n",
        "    N, D = ... # TODO Number of datapoints and dimension of datapoint\n",
        "    K = ...  # TODO number of mixtures\n",
        "    Nk = ... # TODO Sum along first axis \n",
        "    Mu = ... # TODO\n",
        "    Sigma = ... # TODO\n",
        "\n",
        "    # TODO: fill in Sigma[k] for each k\n",
        "\n",
        "    Pi = ... # TODO\n",
        "    return Mu, Sigma, Pi"
      ],
      "metadata": {
        "id": "Ly4KBK5S8zX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d) [4pts]** Run this cell to call the Gaussian Mixture EM algorithm."
      ],
      "metadata": {
        "id": "Y2uTCffj9LX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D = # TODO\n",
        "K = # TODO\n",
        "Mu = np.zeros([D, K])\n",
        "Mu[:, 1] = 1.\n",
        "Sigma = [np.eye(2), np.eye(2)]\n",
        "Pi = np.ones(K) / K\n",
        "Gamma = np.zeros([N, K]) # Gamma is the matrix of responsibilities \n",
        "\n",
        "max_iter  = 200\n",
        "\n",
        "for it in range(max_iter):\n",
        "    Gamma = # TODO\n",
        "    Mu, Sigma, Pi = # TODO\n",
        "\n",
        "class_1 = # TODO\n",
        "class_2 = # TODO"
      ],
      "metadata": {
        "id": "ElKyY11N9KmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e) [2pts]** Make a scatterplot for the data points showing the Gaussian Mixture cluster assignments of each point"
      ],
      "metadata": {
        "id": "DDGgFxSt9sp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(...) # TODO first class, x shape\n",
        "plt.plot(...) # TODO second class, circle shape"
      ],
      "metadata": {
        "id": "k_AU2yu_9w49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 [8pts] Comment on findings + additional experiments\n",
        "\n",
        "Comment on the results:\n",
        "\n",
        "* Compare the performance of k-Means and EM based on the resulting cluster assignments. [2pts]\n",
        "* Compare the performance of k-Means and EM based on their convergence rate. What is the bottleneck for which method? [2pts]\n",
        "* Experiment with 5 different data realizations (generate new data), run your algorithms, and summarize your findings. Does the algorithm performance depend on different realizations of data? [3pts]\n",
        "* Comment on what might happen as you increase the number $K$ of clusters. [1pts]\n",
        " \n"
      ],
      "metadata": {
        "id": "QFy3oFR57Nqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here."
      ],
      "metadata": {
        "id": "wSsvAFJq7WSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here, if you have any"
      ],
      "metadata": {
        "id": "N3qKP6ya7RJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}